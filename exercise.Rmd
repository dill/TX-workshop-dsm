---
title: "Density surface models: example analysis"
author: David L Miller
output:
  xaringan::moon_reader:
    chakra: libs/remark-latest.min.js
    css: ["default", "fonts", "custom.css"]
    nature:
      highlightStyle: github
      highlightLines: true
---

background-size: cover

```{r include=FALSE}
library(knitr)
opts_chunk$set(cache=TRUE, echo=FALSE, message=FALSE,
               warnings=FALSE, errors=FALSE)
```

```{r datasetup}
species.code <- 'w'
birds <- c("c", "g", "r", "w")
species.names <- c("chaffinch", "great tit", "robin", "winter wren")
species.number <- match(species.code, birds)
is.chaf <- ifelse(species.code=='c', TRUE, FALSE)
is.tit <- ifelse(species.code=='g', TRUE, FALSE)
is.robin <- ifelse(species.code=='r', TRUE, FALSE)
is.wren <- ifelse(species.code=='w', TRUE, FALSE)
design.estimates.species <- c(0.9, 0.22, 0.6, 1.02)
# load raw data
data <- read.csv("data/montrave-point.csv", header = TRUE)
data <- subset(data, species==species.code & distance<=110)
stn.coords <- read.csv("data/montrave-stations.csv")
stn.coords <- stn.coords[, c(1,3,4)]
names(stn.coords) <- c("Ptname","x","y")
data.with.locations <- merge(data, stn.coords,
                             by.x="Sample.Label", by.y="Ptname", all.x=TRUE)
library("rgdal")
library("maptools")
library("ggplot2")
library("plyr")

# provide the correct projection for the data
#newproj <- "+proj=lcc +nadgrids=ntf_r93.gsb,null +a=6378249.2000 +rf=293.4660210000000  +pm=2.337229167 +lat_0=46.800000000 +lon_0=0.000000000 +k_0=0.99987742 +lat_1=46.800000000 +x_0=600000.000 +y_0=200000.000 +units=m +no_defs"
# import shapefile for the survey area
shape <- suppressWarnings(readShapeSpatial("data/montrave-studyarea.shp", #proj4string = CRS(newproj),
                          repair=TRUE, force_ring=TRUE))
# import shapefile for the points
stations <- suppressWarnings(readShapeSpatial("data/montrave-stations.shp", #proj4string = CRS(newproj),
                        repair=TRUE, force_ring=TRUE))
station.coords <- data.frame(x=stations@data$X..Easting,
                             y=stations@data$Y..Northin)

# make the object simpler
survey.area <- data.frame(shape@polygons[[1]]@Polygons[[1]]@coords)
names(survey.area) <- c("x","y")

```




# Example analysis

.pull-left[
- Point transect data
- Collected by Steve Buckland
- Data on chaffinch, great tit, robin, winter wren
- Just looking at wren (*Troglodytes troglodytes*) here
]

.pull-right[
![a winter wren](36408899245_0401014071_b.jpg)
]


---

# Sampled locations

```{r map_1, fig.cap="Map with sampled points",fig.align='center'}
# produce a map of the survey area with all the point sampled
p <- qplot(data=survey.area, x=x, y=y, geom="polygon", fill=I("lightblue"), ylab="y", xlab="x", alpha=I(0.7)) +
  geom_point(aes(x=x, y=y, group="Point"), data=station.coords, colour="darkblue") +
  coord_equal() +
  theme_minimal()
print(p)

```



Setting up the segment data, which in our case are the points that were visited...


```{r segdata}
# construct segment data (x, y, Effort, Sample.Label)
segdata <- as.data.frame(matrix(NA, ncol = 5, nrow=dim(data.with.locations)[1]))
segdata <- data.with.locations[, c("Sample.Label", "Effort",  
                                   "Sample.Label", "x", "y")]
segdata <- segdata[!duplicated(segdata), ]
colnames(segdata) <- c("Sample.Label", "Effort", "Segment.Label", "X", "Y")
```



Setting up the observation data, which links the observations with the segments (points):

```{r obsdata}
obsdata <- data.with.locations
obsdata$size <- 1
obsdata$object <- 1:nrow(obsdata)
```

---


# Prediction grid

```{r proj_grid, fig.cap="Projection grid",fig.align='center', fig.height=3}
# create a prediction grid
# method from http://rfunctions.blogspot.co.uk/2014/12/how-to-create-grid-and-intersect-it.html
library("raster")
library("rgeos")
library("dismo")

# Create an empty raster
grid <- raster(extent(shape))
# Choose its resolution. 50 m in both X and Y (truncation distance)
res(grid) <- 50
# Make the grid have the same coordinate reference system (CRS) as the shapefile.
proj4string(grid) <- proj4string(shape)
# Transform this raster into a polygon and you will have a grid
gridpolygon <- rasterToPolygons(grid)
# Intersect our grid with shape
pred.grid <- intersect(shape, gridpolygon)
# Plot the intersected shape to check if everything is fine.
plot(pred.grid)

# make pred.grid of type point rather than polygon for export as point shapefile
point.shape.for.export <- data.frame(x=coordinates(pred.grid)[,1],
                                     y=coordinates(pred.grid)[,2],
                                     area=rep(2500, times=dim(pred.grid@data)[1]))
coordinates(point.shape.for.export) <- c("x", "y")
class(point.shape.for.export)
writeSpatialShape(x=point.shape.for.export, fn="point-export")

# create the data.frame for prediction
preddata <- as.data.frame(matrix(NA, ncol=3, nrow=dim(pred.grid@data)[1]))
colnames(preddata) <- c("X", "Y", "area")
for (i in 1:dim(pred.grid@data)[1]){
  preddata[i, c("X", "Y")] <- pred.grid@polygons[[i]]@labpt
  preddata[i, c("area")] <- pred.grid@polygons[[i]]@area
}
```

---

# Detection function

```{r det_function}
library("Distance")
df.ht <- ds(data.with.locations, transect="point", truncation = 110, 
            formula=~1, key="hr", adjustment=NULL)
plot(df.ht, pdf=TRUE)
```

---

# Goodness-of-fit


.pull-left[
```{r gof, eval=TRUE}
gof.results <- gof_ds(df.ht, plot = FALSE)
gof.cvm <- round(gof.results$dsgof$CvM$W, 3)
gof.Pval <- round(gof.results$dsgof$CvM$p, 3)
gofstats <- paste("CvM W=", gof.cvm, "\nP=", gof.Pval)
#text(label=gofstats, x=80, y=.015, cex=0.7)
gof_ds(df.ht)
```
]

.pull-right[
- Cramer-von Mises $p$-value = `r gof.Pval`
]
---

class: inverse, middle, center

# We can now fit some DSMs...


---

# Density surface modelling


- Only have $x$ and $y$ (projected) locations
- $$\mathbb{E} (n_j) = \pi w^2 \hat{p}_j \exp\left[ \beta_0 + s_{x,y}(x_j, y_j) \right]$$
- formula in Distance for Windows: `s(X, Y, k=25)`
- (need to test `k` values, just need to be too big)


```{r dsmload}
library("dsm")
```




```{r dsm-fitting}
mod_tw <- dsm(count~s(X, Y, k=25), ddf.obj=df.ht, segment.data=segdata, 
              observation.data=obsdata, family=tw(), transect="point")
```

---

# Model results

```{r summary}
summary(mod_tw)
```

---

# Model checking plots

```{r check, fig.width=7, fig.height=7}
gam.check(mod_tw)
```

---

# Checking basis size

```{r check2, fig.keep="none"}
gam.check(mod_tw)
```

---

## Making predictions


```{r makepred}
mod_tw_pred <- predict(mod_tw, preddata, preddata$area)
```

```{r plotpred}
grid_plot_obj <- function(shape,fill, name){

  # what data were supplied?
  names(fill) <- NULL
  row.names(fill) <- NULL
  data <- data.frame(fill)
  names(data) <- name

  # ! need to give the right name of the shapefile
  sp <- shape
  spdf <- SpatialPolygonsDataFrame(sp, data)
  spdf@data$id <- rownames(spdf@data)
  spdf.points <- fortify(spdf, region="id")
  spdf.df <- join(spdf.points, spdf@data, by="id")

  # store the x/y even when projected and labelled as "long" and "lat"
  spdf.df$x <- spdf.df$long
  spdf.df$y <- spdf.df$lat

  geom_polygon(aes_string(x="x",y="y",fill=name, group="group"), data=spdf.df)
}

# make the plots
pcount_tw <- ggplot() + grid_plot_obj(pred.grid, mod_tw_pred, "Count") + 
  scale_fill_gradient(low="white", high="chocolate4")  +
  coord_equal() + theme_minimal() +
  geom_path(aes(x=x, y=y), data=survey.area) +
  geom_point(aes(x = x, y = y, group="Point"), data = data.with.locations, colour = "black") +
  labs(fill="Count")
print(pcount_tw)
```

---

# Uncertainty in predictions

```{r cvpred}
# data setup
preddata.var <- split(preddata, 1:nrow(preddata))

mod_tw_vargam <- dsm.var.gam(mod_tw, pred.data=preddata.var, off.set=preddata$area)
pcount_cv_tw <- ggplot() + grid_plot_obj(pred.grid,
                                        sqrt(mod_tw_vargam$pred.var)/unlist(mod_tw_vargam$pred),"CV") + 
  scale_fill_gradient(low = "white", high = "chocolate4") +
  coord_equal() + theme_minimal() +
  geom_path(aes(x=x, y=y),data=survey.area) +
  geom_point(aes(x=x, y=y, group="Point"), data=data.with.locations, colour="black") +
  labs(fill="CV")
print(pcount_cv_tw)
```

---


# Comparison with design-based estimates

- Buckland (2006) analysed these data w/ design-based method

```{r den.hectare, results="asis"}
sq.m.to.ha <- 100*100
study.area.ha <- shape@polygons[[1]]@area / sq.m.to.ha
total.abundance.tw <- sum(mod_tw_pred)
density.ha.tw <- total.abundance.tw/study.area.ha
est.table <- data.frame(Density=c(density.ha.tw,
                                  design.estimates.species[species.number]))
# TKTKTK do uncertainty?
row.names(est.table) <- c("DSM", "Design-based")
knitr::kable(est.table, digits=3, format="markdown")
```


# Summary



