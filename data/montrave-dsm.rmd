---
title: "DSM using point transect data"
author: "Rexstad"
date: "December 2017"
output:
  pdf_document: default
  html_document: default
subtitle: Buckland's Montrave songbird dataset
---

```{r echo=FALSE}
library(knitr)
opts_chunk$set(comment=NA, message=FALSE, error=FALSE, warning=FALSE, cache=FALSE)
species.code <- 'c'
birds <- c("c", "g", "r", "w")
species.names <- c("chaffinch", "great tit", "robin", "winter wren")
species.number <- match(species.code, birds)
is.chaf <- ifelse(species.code=='c', TRUE, FALSE)
is.tit <- ifelse(species.code=='g', TRUE, FALSE)
is.robin <- ifelse(species.code=='r', TRUE, FALSE)
is.wren <- ifelse(species.code=='w', TRUE, FALSE)
design.estimates.species <- c(0.9, 0.22, 0.6, 1.02)
```

## Observation data and covariates

First, we load sightings data and subset data to analyse the selected species: `r species.names[species.number]`.

```{asis, echo=is.chaf}
![](img/chaffinch.PNG)
```

```{asis, echo=is.tit}
![](img/greattit.PNG)
```

```{asis, echo=is.robin}
![](img/robin.PNG)
```
```{asis, echo=is.wren}
![](img/wren.PNG)
```


```{r data}
# load raw data
data <- read.csv("montrave-point.csv", header = TRUE)
data <- subset(data, species==species.code & distance<=110)
str(data)
```

Then, we build our dataset for the detection function. Note that the distance are given in metres, so density would be estimated as the number of individuals per square metre. 

Note that the `Effort` column contains `2`, as each point was visited twice.  

### Station data

This is file I created containing both station ID and coordinates.  Merge this file with the observation file to get coordinates into the observation file.

```{r coord-merge}
stn.coords <- read.csv("montrave-stations.csv")
stn.coords <- stn.coords[, c(1,3,4)]
names(stn.coords) <- c("Ptname","x","y")
data.with.locations <- merge(data, stn.coords, 
                             by.x="Sample.Label", by.y="Ptname", all.x=TRUE)
```

### GIS data
A Google Earth representation of the study area

![](img/studyarea.PNG)

```{r map_data}
library("rgdal")
library("maptools")
library("ggplot2")
library("plyr")

# provide the correct projection for the data
#newproj <- "+proj=lcc +nadgrids=ntf_r93.gsb,null +a=6378249.2000 +rf=293.4660210000000  +pm=2.337229167 +lat_0=46.800000000 +lon_0=0.000000000 +k_0=0.99987742 +lat_1=46.800000000 +x_0=600000.000 +y_0=200000.000 +units=m +no_defs"
# import shapefile for the survey area
shape <- readShapeSpatial("montrave-studyarea.shp", #proj4string = CRS(newproj),
                          repair=TRUE, force_ring=TRUE, verbose=TRUE)
# import shapefile for the points
stations <- readShapeSpatial("montrave-stations.shp", #proj4string = CRS(newproj),
                        repair=TRUE, force_ring=TRUE, verbose=TRUE)
station.coords <- data.frame(x=stations@data$X..Easting, 
                             y=stations@data$Y..Northin)

# make the object simpler
survey.area <- data.frame(shape@polygons[[1]]@Polygons[[1]]@coords)
names(survey.area) <- c("x","y")
```

We can then produce a map of the Montrave wooded region with the sampled points:

```{r map_1, fig.cap="Map with sampled points",fig.align='center'}
# produce a map of the survey area with all the point sampled
p <- qplot(data=survey.area, x=x, y=y, geom="polygon", fill=I("lightblue"), ylab="y", xlab="x", alpha=I(0.7)) +
  geom_point(aes(x=x, y=y, group="Point"), data=station.coords, colour="darkblue") +
  coord_equal() +
  theme_minimal()
print(p)

```

Setting up the segment data, which in our case are the points that were visited...


```{r segdata}
# construct segment data (x, y, Effort, Sample.Label)
segdata <- as.data.frame(matrix(NA, ncol = 5, nrow=dim(data.with.locations)[1]))
segdata <- data.with.locations[, c("Sample.Label", "Effort",  
                                   "Sample.Label", "x", "y")]
segdata <- segdata[!duplicated(segdata), ]
colnames(segdata) <- c("Sample.Label", "Effort", "Segment.Label", "X", "Y")
```



Setting up the observation data, which links the observations with the segments (points):

```{r obsdata}
obsdata <- data.with.locations
obsdata$size <- 1
obsdata$object <- 1:nrow(obsdata)
```


We then create the prediction grid:

```{r proj_grid, fig.cap="Projection grid",fig.align='center', fig.height=3}
# create a prediction grid
# method from http://rfunctions.blogspot.co.uk/2014/12/how-to-create-grid-and-intersect-it.html
library("raster")
library("rgeos")
library("dismo")

# Create an empty raster
grid <- raster(extent(shape))
# Choose its resolution. 50 m in both X and Y (truncation distance)
res(grid) <- 50
# Make the grid have the same coordinate reference system (CRS) as the shapefile.
proj4string(grid) <- proj4string(shape)
# Transform this raster into a polygon and you will have a grid
gridpolygon <- rasterToPolygons(grid)
# Intersect our grid with shape
pred.grid <- intersect(shape, gridpolygon)
# Plot the intersected shape to check if everything is fine.
plot(pred.grid)

# make pred.grid of type point rather than polygon for export as point shapefile
point.shape.for.export <- data.frame(x=coordinates(pred.grid)[,1],
                                     y=coordinates(pred.grid)[,2],
                                     area=rep(2500, times=dim(pred.grid@data)[1]))
coordinates(point.shape.for.export) <- c("x", "y")
class(point.shape.for.export)
writeSpatialShape(x=point.shape.for.export, fn="point-export")

# create the data.frame for prediction
preddata <- as.data.frame(matrix(NA, ncol=3, nrow=dim(pred.grid@data)[1]))
colnames(preddata) <- c("X", "Y", "area")
for (i in 1:dim(pred.grid@data)[1]){
  preddata[i, c("X", "Y")] <- pred.grid@polygons[[i]]@labpt
  preddata[i, c("area")] <- pred.grid@polygons[[i]]@area
}
```

## Detection function

Detection functions can be fitted using the `Distance` R package:

```{r det_function}
library("Distance")
df.ht <- ds(data.with.locations, transect="point", truncation = 110, 
            formula=~1, key="hr", adjustment=NULL)
```



```{r gof, eval=TRUE}
plot(df.ht, pdf=TRUE)
gof.results <- gof_ds(df.ht, plot = FALSE)
gof.cvm <- round(gof.results$dsgof$CvM$W, 3)
gof.Pval <- round(gof.results$dsgof$CvM$p, 3)
gofstats <- paste("CvM W=", gof.cvm, "\nP=", gof.Pval)
text(label=gofstats, x=80, y=.015, cex=0.7)
```


We can now fit some DSMs...


## Density surface modelling

```{r dsmload}
library("dsm")
```

Now fitting the DSMs, we model the count as a function of space, using the Tweedie (`tw()`) response distribution:

```{r dsm-fitting}
mod_tw <- dsm(count~s(X, Y, k=25), ddf.obj=df.ht, segment.data=segdata, 
              observation.data=obsdata, family=tw(), transect="point")
flat.model <- dsm(count~1, ddf.obj=df.ht, segment.data=segdata, 
              observation.data=obsdata, family=tw(), transect="point")
```

```{r gam check, fig.cap="Results from gam check", fig.align='center', fig.width=10, fig.height=10}
summary(mod_tw)
gam.check(mod_tw)
```

## Making predictions

We can now predict over the prediction grid.

```{r makepred}
mod_tw_pred <- predict(mod_tw, preddata, preddata$area)
flat.predict <- predict(flat.model, preddata, preddata$area)
```

Here we define a convenience function to generate an appropriate data structure for `ggplot2` to plot: given the argument `fill` (the covariate vector to use as the fill) and a name, return a geom_polygon object  (fill must be in the same order as the polygon data).

```{r plotpred}
grid_plot_obj <- function(shape,fill, name){
  
  # what data were supplied?
  names(fill) <- NULL
  row.names(fill) <- NULL
  data <- data.frame(fill)
  names(data) <- name
  
  # ! need to give the right name of the shapefile
  sp <- shape
  spdf <- SpatialPolygonsDataFrame(sp, data)
  spdf@data$id <- rownames(spdf@data)
  spdf.points <- fortify(spdf, region="id")
  spdf.df <- join(spdf.points, spdf@data, by="id")
  
  # store the x/y even when projected and labelled as "long" and "lat"
  spdf.df$x <- spdf.df$long
  spdf.df$y <- spdf.df$lat
  
  geom_polygon(aes_string(x="x",y="y",fill=name, group="group"), data=spdf.df)
}

# make the plots
pcount_tw <- ggplot() + grid_plot_obj(pred.grid, mod_tw_pred, "Count") + 
  scale_fill_gradient(low="white", high="chocolate4")  +
  coord_equal() + theme_minimal() +
  geom_path(aes(x=x, y=y), data=survey.area) +
  geom_point(aes(x = x, y = y, group="Point"), data = data.with.locations, colour = "black") +
  labs(fill="Count")
```

We can also estimate uncertainty in our abundance map in the form of a map of coefficients of variation:

```{r cvpred}
# data setup
preddata.var <- split(preddata, 1:nrow(preddata))

mod_tw_vargam <- dsm.var.gam(mod_tw, pred.data=preddata.var, off.set=preddata$area)
pcount_cv_tw <- ggplot() + grid_plot_obj(pred.grid,
                                        sqrt(mod_tw_vargam$pred.var)/unlist(mod_tw_vargam$pred),"CV") + 
  scale_fill_gradient(low = "white", high = "chocolate4") +
  coord_equal() + theme_minimal() +
  geom_path(aes(x=x, y=y),data=survey.area) +
  geom_point(aes(x=x, y=y, group="Point"), data=data.with.locations, colour="black") +
  labs(fill="CV")
```


```{r, fig.cap="Map of bird  density in Montrave study area and of associated coefficient of variation from the DSM model with coordinates as covariates using Tweedie response. Black dots represent the sampled point. Units of measure are abundance of birds within a 50x50m prediction grid cell.  ", fig.align='center', fig.width=10}
library("gridExtra")
grid.arrange(pcount_tw, pcount_cv_tw, ncol=2)
```

## Comparison with design-based estimates

Buckland (2006) analysed these snapshot method data using design-based method and produced these density estimates measured in units of birds per hectare. 

![](img/all-design-estimates.PNG)

Converting the above density map to a density estimate requires a bit of work.

```{r den.hectare}
sq.m.to.ha <- 100*100
study.area.ha <- shape@polygons[[1]]@area / sq.m.to.ha
total.abundance.tw <- sum(mod_tw_pred)
density.ha.tw <- total.abundance.tw/study.area.ha
density.ha.flat <- sum(flat.predict)/study.area.ha
est.table <- data.frame(estimates=c(density.ha.flat, density.ha.tw, 
                                    design.estimates.species[species.number]))
row.names(est.table) <- c("s(x,y)", "~1", "design")
knitr::kable(est.table, digits=3, 
             caption="Comparison of density estimates (indiv/ha) from a) DSM model using x,y as predictors, b) flat density surface and c) design-based estimator." )
```

This is a small example of what can be done using DSM and the simplest covariates available: geographical coordinates.

# References

* Buckland, S.T. 2006. Point-transect surveys for songbirds: robust methodologies. The Auk **123** :345-57.

* Miller, D. L., M. L. Burt, E. A. Rexstad, and L. Thomas. **2013**. Spatial models for distance sampling data : recent developments and future directions. Methods in Ecology and Evolution, **4** :1001--1010.
 * R Core Team. **2016**. R : A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria.
